{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark3</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark3</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'executorCores': 4, 'numExecutors': 13, 'proxyUser': 'jovyan', 'kind': 'pyspark3'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark3</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"executorCores\":4,\"numExecutors\":13}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = 's3://srdata-lab/Matt/DeepARInputs/deepar_input/hivejob0'\n",
    "paths    =['s3://srdata-lab/Matt/DeepARInputs/deepar_input/hivejob0/*tuning_evnt_start_dt=*']\n",
    "df=spark.read.option(\"basePath\",basePath).parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Subset markets to periods of \"good\" data\n",
    "df=df.where((col(\"mkt_cd\").isin([\"501\",  \"635\"]))\n",
    "           |((col(\"mkt_cd\").isin([\"539\"])) & (df.tuning_evnt_start_dt >= \"2017-04-01\"))\n",
    "           | ((col(\"mkt_cd\").isin([\"609\"])) & (df.tuning_evnt_start_dt >= \"2018-01-01\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('mkt_cd').agg(f.min('tuning_evnt_start_dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+----------------------+-------------------------+\n",
      "|mkt_cd|count(DISTINCT syscode)|count(DISTINCT stn_id)|min(tuning_evnt_start_dt)|\n",
      "+------+-----------------------+----------------------+-------------------------+\n",
      "|   635|                      6|                   116|               2016-09-01|\n",
      "|   539|                     11|                   121|               2017-04-01|\n",
      "|   609|                     13|                   106|               2018-01-01|\n",
      "|   501|                     12|                   123|               2016-09-01|\n",
      "+------+-----------------------+----------------------+-------------------------+"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.groupby('mkt_cd').agg(f.countDistinct('syscode'),f.countDistinct('stn_id'),f.min('tuning_evnt_start_dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Add MarchMad and CFBNT indicators\n",
    "# df = df.withColumn(\"MarchMadInd\",\n",
    "#                    f.when((df[\"tuning_evnt_start_dt\"] >= '2018-03-15' ) & (df[\"tuning_evnt_start_dt\"] <= '2018-04-10'), 1)\n",
    "#                     .otherwise(0))\n",
    "\n",
    "# df.groupby('MarchMadInd').agg(f.countDistinct('tuning_evnt_start_dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"CFBNT_Ind\",\n",
    "#                    f.when((df[\"tuning_evnt_start_dt\"] >= '2018-01-06' ) & (df[\"tuning_evnt_start_dt\"] <= '2018-01-10'), 1)\n",
    "#                     .otherwise(0))\n",
    "\n",
    "# df.groupby('CFBNT_Ind').agg(f.countDistinct('tuning_evnt_start_dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+\n",
      "|Syscode_Subsc_Buckt|count(DISTINCT syscode)|\n",
      "+-------------------+-----------------------+\n",
      "|                  0|                     42|\n",
      "|                  1|                      3|\n",
      "|                  2|                      6|\n",
      "|                  3|                      5|\n",
      "|                  4|                      6|\n",
      "|                  5|                      3|\n",
      "|                  6|                     18|\n",
      "+-------------------+-----------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "import pyspark.sql.functions as f\n",
    "# Sysocode Subscriber Buckets\n",
    "# Sagemaker can't train on cat fields with missing attributes (since 6th and 7th buckets were missing, training threw an error)\n",
    "# Limit buckets to 6 total\n",
    "df = df.withColumn(\"Syscode_Subsc_Buckt\",\n",
    "                   f.when((df[\"sub_cnt\"] <= 5000 ) & (df[\"sub_cnt\"] > 0), 0)\n",
    "                    .when((df[\"sub_cnt\"] <= 10000 ) & (df[\"sub_cnt\"] > 5000), 1)\n",
    "                    .when((df[\"sub_cnt\"] <= 20000 ) & (df[\"sub_cnt\"] > 10000), 2)\n",
    "                    .when((df[\"sub_cnt\"] <= 30000 ) & (df[\"sub_cnt\"] > 20000), 3)\n",
    "                    .when((df[\"sub_cnt\"] <= 40000 ) & (df[\"sub_cnt\"] > 30000), 4)\n",
    "                    .when((df[\"sub_cnt\"] <= 50000 ) & (df[\"sub_cnt\"] > 40000), 5)\n",
    "                    .when((df[\"sub_cnt\"] > 50000 ), 6)\n",
    "                    .otherwise(0)) #if NULLs exist, place them in 0th bucket\n",
    "\n",
    "df.groupby('Syscode_Subsc_Buckt').agg(f.countDistinct('syscode')).sort('Syscode_Subsc_Buckt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('mindate', df['mindate'].cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[daypart_hour: int, syscode: string, stn_id: string, Syscode_Stn: double, mindate: string, stn_qlty_cd: string, ntwrk_genre_cd: string, ntwrk_genre_recd: string, stn_norm_nm: string, mkt_cd: string, total_events: double, spp_impressions: double, sub_cnt: int, tuning_evnt_start_dt: date, Syscode_Subsc_Buckt: int]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df = df.withColumn('Syscode_Stn', F.concat(F.col('syscode'), F.lit(\" - \"), F.col('stn_id')))\n",
    "df.withColumn(\"Syscode_Stn\", df[\"Syscode_Stn\"].cast(DoubleType()).alias(\"Syscode_Stn\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns=[\"Syscode_Stn\", \"tuning_evnt_start_dt\", \"daypart_hour\", \"mindate\",\"stn_qlty_cd\",\"ntwrk_genre_recd\",\n",
    "              \"Syscode_Subsc_Buckt\",\"spp_impressions\"] #\"MarchMadInd\",\"CFBNT_Ind\"\n",
    "\n",
    "df=df.select([column for column in df.columns if column in keep_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+----------------+--------------------+------------------+-------------------+\n",
      "| Syscode_Stn|            mindate|stn_qlty_cd|ntwrk_genre_recd|tuning_evnt_start_dt|     spp_imp_daily|Syscode_Subsc_Buckt|\n",
      "+------------+-------------------+-----------+----------------+--------------------+------------------+-------------------+\n",
      "|3466 - 14899|2017-03-07 00:00:00|         SD|          SPORTS|          2018-03-12|5.1591689999999994|                  6|\n",
      "|1299 - 64549|2016-09-01 00:00:00|         HD|            NEWS|          2018-03-12|         91.556397|                  1|\n",
      "|6487 - 10222|2016-09-02 00:00:00|         SD|         SPANISH|          2018-03-12|            0.0175|                  2|\n",
      "|3370 - 61854|2017-10-28 00:00:00|         HD|          SPORTS|          2018-03-12|448.64582099999996|                  4|\n",
      "|3370 - 59432|2017-10-28 00:00:00|         HD|            KIDS|          2018-03-12|3914.1021940000005|                  4|\n",
      "+------------+-------------------+-----------+----------------+--------------------+------------------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Aggregate at Daily level\n",
    "# Since 0-impression records are getting assigned \"Syscode_Subsc_Buckt\"=0, take the max(\"Syscode_Subsc_Buckt\")\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.groupBy(\"Syscode_Stn\", \"mindate\",\"stn_qlty_cd\",\"ntwrk_genre_recd\",\"tuning_evnt_start_dt\").agg(f.sum(\"spp_impressions\").alias('spp_imp_daily'), f.max(\"Syscode_Subsc_Buckt\").alias(\"Syscode_Subsc_Buckt\"))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+----------------+--------------------+------------------+-------------------+\n",
      "| Syscode_Stn|            mindate|stn_qlty_cd|ntwrk_genre_recd|tuning_evnt_start_dt|     spp_imp_daily|Syscode_Subsc_Buckt|\n",
      "+------------+-------------------+-----------+----------------+--------------------+------------------+-------------------+\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-01| 89.31305300000002|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-02|123.86610299999998|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-03|        121.907775|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-04|164.37999100000002|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-05|210.98889800000003|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-06|180.09388399999997|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-07|218.93444300000002|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-08|        121.805556|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-09|118.97193899999999|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-10|        110.632229|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-11|        156.033887|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-12|        143.826943|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-13|        426.222781|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-14|        179.620842|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-15|        143.559434|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-16|        192.003342|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-17|150.68527699999999|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-18|127.77444500000003|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-19|        100.247229|                  3|\n",
      "|0318 - 10021|2017-10-28 00:00:00|         SD| Movies & Series|          2018-01-20|165.40360900000002|                  3|\n",
      "+------------+-------------------+-----------+----------------+--------------------+------------------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df.orderBy([\"Syscode_Stn\",\"tuning_evnt_start_dt\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Syscode_Stn,StringType,true),StructField(mindate,StringType,true),StructField(stn_qlty_cd,StringType,true),StructField(ntwrk_genre_recd,StringType,true),StructField(tuning_evnt_start_dt,DateType,true),StructField(spp_imp_daily,DoubleType,true),StructField(Syscode_Subsc_Buckt,IntegerType,true),StructField(stn_qlty_index,IntegerType,false),StructField(ntwrk_genre_index,IntegerType,true)))"
     ]
    }
   ],
   "source": [
    "# Encode cat field as 0-based sequence of positive integers\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import isnan\n",
    "df = df.withColumn(\"stn_qlty_index\",\n",
    "                   f.when(df[\"stn_qlty_cd\"] == 'HD',0)\n",
    "                    .otherwise(1))\n",
    "\n",
    "df = df.withColumn(\"ntwrk_genre_index\",\n",
    "                   f.when(df[\"ntwrk_genre_recd\"] == 'SPORTS',0)\n",
    "                    .when(df[\"ntwrk_genre_recd\"] == 'NEWS',1)\n",
    "                    .when(df[\"ntwrk_genre_recd\"] == 'SPANISH',2)\n",
    "                    .when(df[\"ntwrk_genre_recd\"] == 'Major Sports & Misc',3)\n",
    "                    .when(df[\"ntwrk_genre_recd\"].isNull(), 4)\n",
    "                    .when(df[\"ntwrk_genre_recd\"] == 'Movies & Series',5)\n",
    "                    .when(df[\"ntwrk_genre_recd\"] == 'Music',6)\n",
    "                    .when(df[\"ntwrk_genre_recd\"] == 'KIDS',7))\n",
    "\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------------------+\n",
      "|ntwrk_genre_index|count(DISTINCT Syscode_Stn)|\n",
      "+-----------------+---------------------------+\n",
      "|                0|                        693|\n",
      "|                1|                        563|\n",
      "|                2|                        176|\n",
      "|                3|                        187|\n",
      "|                4|                       1984|\n",
      "|                5|                        559|\n",
      "|                6|                        162|\n",
      "|                7|                        280|\n",
      "+-----------------+---------------------------+"
     ]
    }
   ],
   "source": [
    "df.groupby('ntwrk_genre_index').agg(f.countDistinct('Syscode_Stn')).sort('ntwrk_genre_index').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Full minus (total-2) months for training; testing data should be full data\n",
    "train_data = df.filter(df.tuning_evnt_start_dt <= \"2018-04-30\")\n",
    "test_data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  month| count|\n",
      "+-------+------+\n",
      "|2016-09| 58806|\n",
      "|2016-10| 61380|\n",
      "|2016-11| 59400|\n",
      "|2016-12| 61380|\n",
      "|2017-01| 61380|\n",
      "|2017-02| 55440|\n",
      "|2017-03| 61380|\n",
      "|2017-04| 95850|\n",
      "|2017-05| 99220|\n",
      "|2017-06| 96426|\n",
      "|2017-07|100334|\n",
      "|2017-08|102605|\n",
      "|2017-09| 99369|\n",
      "|2017-10|102703|\n",
      "|2017-11| 99495|\n",
      "|2017-12|102827|\n",
      "|2018-01|144460|\n",
      "|2018-02|130480|\n",
      "|2018-03|144461|\n",
      "|2018-04|139870|\n",
      "+-------+------+"
     ]
    }
   ],
   "source": [
    "train_data.select(f.date_format('tuning_evnt_start_dt','yyyy-MM').alias('month')).groupby('month').count().sort('month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|  month| count|\n",
      "+-------+------+\n",
      "|2016-09| 58806|\n",
      "|2016-10| 61380|\n",
      "|2016-11| 59400|\n",
      "|2016-12| 61380|\n",
      "|2017-01| 61380|\n",
      "|2017-02| 55440|\n",
      "|2017-03| 61380|\n",
      "|2017-04| 95850|\n",
      "|2017-05| 99220|\n",
      "|2017-06| 96426|\n",
      "|2017-07|100334|\n",
      "|2017-08|102605|\n",
      "|2017-09| 99369|\n",
      "|2017-10|102703|\n",
      "|2017-11| 99495|\n",
      "|2017-12|102827|\n",
      "|2018-01|144460|\n",
      "|2018-02|130480|\n",
      "|2018-03|144461|\n",
      "|2018-04|139870|\n",
      "|2018-05|144600|\n",
      "|2018-06|140058|\n",
      "+-------+------+"
     ]
    }
   ],
   "source": [
    "test_data.select(f.date_format('tuning_evnt_start_dt','yyyy-MM').alias('month')).groupby('month').count().sort('month').show(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----+----+----+\n",
      "|              start|              target|cat1|cat2|cat3|\n",
      "+-------------------+--------------------+----+----+----+\n",
      "|2017-10-28 00:00:00|[827.80993, 965.9...|   0|   4|   3|\n",
      "|2016-09-01 00:00:00|[1.655002, 7.3641...|   1|   4|   0|\n",
      "|2016-09-01 00:00:00|[1.813888, 10.992...|   1|   4|   0|\n",
      "|2016-09-01 00:00:00|[1827.48557300000...|   0|   1|   2|\n",
      "|2017-03-15 00:00:00|[5.965, 13.972223...|   1|   7|   6|\n",
      "|2017-03-15 00:00:00|[1201.162801, 676...|   0|   4|   6|\n",
      "|2017-03-07 00:00:00|[1964.33801099999...|   0|   4|   6|\n",
      "|2017-03-07 00:00:00|[21.3363869999999...|   1|   4|   6|\n",
      "|2017-03-07 00:00:00|[1574.029682, 122...|   0|   4|   6|\n",
      "|2017-10-28 00:00:00|[251.058605000000...|   1|   1|   5|\n",
      "|2017-10-28 00:00:00|[338.069443000000...|   1|   4|   4|\n",
      "|2017-10-28 00:00:00|[2190.96271900000...|   0|   4|   4|\n",
      "|2017-10-28 00:00:00|[1308.10083399999...|   0|   5|   3|\n",
      "|2016-09-01 00:00:00|[8.71139000000000...|   1|   1|   0|\n",
      "|2016-09-01 00:00:00|[0.5475, 0.889167...|   1|   4|   5|\n",
      "|2016-09-01 00:00:00|[0.226389, 9.6286...|   1|   1|   5|\n",
      "|2017-10-28 00:00:00|[1617.18693600000...|   0|   4|   3|\n",
      "|2017-10-28 00:00:00|[59.416113, 86.09...|   1|   4|   2|\n",
      "|2017-03-07 00:00:00|[1987.27808200000...|   0|   1|   6|\n",
      "|2017-10-28 00:00:00|[197.519169999999...|   1|   4|   6|\n",
      "+-------------------+--------------------+----+----+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "model_input_train = (train_data\n",
    "  .groupby(\"Syscode_Stn\")\n",
    "  .agg(F.min(\"MinDate\"),\n",
    "    F.collect_list(\"spp_imp_daily\"),\n",
    "    F.min(\"stn_qlty_index\"),\n",
    "    F.min(\"ntwrk_genre_index\"),\n",
    "    F.min(\"Syscode_Subsc_Buckt\")))\n",
    "    #F.collect_list(\"MarchMadInd\"),\n",
    "    #F.collect_list(\"CFBNT_Ind\")))\n",
    "\n",
    "model_train_w_combo = model_input_train.select(col(\"Syscode_Stn\"),\n",
    "                                 col(\"min(MinDate)\").alias(\"start\"),\n",
    "                                 col(\"collect_list(spp_imp_daily)\").alias(\"target\"), \n",
    "                                 col(\"min(stn_qlty_index)\").alias(\"cat1\"),\n",
    "                                 col(\"min(ntwrk_genre_index)\").alias(\"cat2\"),\n",
    "                                 col(\"min(Syscode_Subsc_Buckt)\").alias(\"cat3\"))\n",
    "                                 #col(\"collect_list(MarchMadInd)\").alias(\"dynamic_feat\"),\n",
    "                                 #col(\"collect_list(CFBNT_Ind)\").alias(\"dynamic_feat2\"))\n",
    "\n",
    "model_train_actual_input = model_input_train.select(col(\"min(MinDate)\").alias(\"start\"),\n",
    "                                 col(\"collect_list(spp_imp_daily)\").alias(\"target\"), \n",
    "                                 col(\"min(stn_qlty_index)\").alias(\"cat1\"),\n",
    "                                 col(\"min(ntwrk_genre_index)\").alias(\"cat2\"),\n",
    "                                 col(\"min(Syscode_Subsc_Buckt)\").alias(\"cat3\"))\n",
    "                                 #col(\"collect_list(MarchMadInd)\").alias(\"dynamic_feat\"),\n",
    "                                 #col(\"collect_list(CFBNT_Ind)\").alias(\"dynamic_feat2\"))\n",
    "\n",
    "\n",
    "model_input_test = (test_data\n",
    "  .groupby(\"Syscode_Stn\")\n",
    "  .agg(F.min(\"MinDate\"),\n",
    "    F.collect_list(\"spp_imp_daily\"),\n",
    "    F.min(\"stn_qlty_index\"),\n",
    "    F.min(\"ntwrk_genre_index\"),\n",
    "    F.min(\"Syscode_Subsc_Buckt\")))\n",
    "    #F.collect_list(\"MarchMadInd\"),\n",
    "    #F.collect_list(\"CFBNT_Ind\"))\n",
    "\n",
    "model_test_w_combo = model_input_test.select(col(\"Syscode_Stn\"),\n",
    "                                 col(\"min(MinDate)\").alias(\"start\"),\n",
    "                                 col(\"collect_list(spp_imp_daily)\").alias(\"target\"), \n",
    "                                 col(\"min(stn_qlty_index)\").alias(\"cat1\"),\n",
    "                                 col(\"min(ntwrk_genre_index)\").alias(\"cat2\"),\n",
    "                                 col(\"min(Syscode_Subsc_Buckt)\").alias(\"cat3\"))\n",
    "                                 #col(\"collect_list(MarchMadInd)\").alias(\"dynamic_feat\"),\n",
    "                                 #col(\"collect_list(CFBNT_Ind)\").alias(\"dynamic_feat2\"))\n",
    "\n",
    "model_test_actual_input = model_input_test.select(col(\"min(MinDate)\").alias(\"start\"),\n",
    "                                 col(\"collect_list(spp_imp_daily)\").alias(\"target\"), \n",
    "                                 col(\"min(stn_qlty_index)\").alias(\"cat1\"),\n",
    "                                 col(\"min(ntwrk_genre_index)\").alias(\"cat2\"),\n",
    "                                 col(\"min(Syscode_Subsc_Buckt)\").alias(\"cat3\"))\n",
    "                                 #col(\"collect_list(MarchMadInd)\").alias(\"dynamic_feat\"),\n",
    "                                 #col(\"collect_list(CFBNT_Ind)\").alias(\"dynamic_feat2\"))\n",
    "\n",
    "\n",
    "model_train_actual_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+\n",
      "|              start|              target|      cat|\n",
      "+-------------------+--------------------+---------+\n",
      "|2017-10-28 00:00:00|[54.84166, 58.684...|[1, 6, 3]|\n",
      "|2016-09-01 00:00:00|[9.77249999999999...|[1, 4, 0]|\n",
      "|2016-09-01 00:00:00|[2709.37080799999...|[0, 1, 2]|\n",
      "|2017-10-28 00:00:00|[100.683337000000...|[1, 4, 2]|\n",
      "|2017-03-15 00:00:00|[0.620834, 1.5358...|[1, 7, 6]|\n",
      "|2017-03-15 00:00:00|[1230.692473, 884...|[0, 4, 6]|\n",
      "|2017-03-07 00:00:00|[1625.81554999999...|[0, 1, 6]|\n",
      "|2017-03-07 00:00:00|[2631.80556199999...|[0, 4, 6]|\n",
      "|2017-03-07 00:00:00|[64.481668, 9.488...|[1, 4, 6]|\n",
      "|2017-03-07 00:00:00|[2159.36526299999...|[0, 4, 6]|\n",
      "|2017-10-28 00:00:00|[217.632774, 234....|[1, 1, 5]|\n",
      "|2017-10-28 00:00:00|[1987.56245299999...|[0, 4, 4]|\n",
      "|2017-10-28 00:00:00|[1371.844955, 135...|[0, 5, 3]|\n",
      "|2016-09-01 00:00:00|[3.290279, 2.2597...|[1, 1, 0]|\n",
      "|2016-09-01 00:00:00|[0.36194400000000...|[1, 4, 5]|\n",
      "|2016-09-01 00:00:00|[1.053889, 7.4197...|[1, 1, 5]|\n",
      "|2017-10-28 00:00:00|[898.108329000000...|[0, 4, 3]|\n",
      "|2017-10-28 00:00:00|[988.546666, 988....|[0, 4, 3]|\n",
      "|2017-10-28 00:00:00|[816.456101000000...|[0, 4, 3]|\n",
      "|2016-09-01 00:00:00|[3.578613, 3.9311...|[1, 4, 0]|\n",
      "+-------------------+--------------------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "train_final_input = model_train_actual_input.select(\"start\",\"target\", f.array([\"cat1\",\"cat2\",\"cat3\"]).alias(\"cat\"))\n",
    "                                                    #f.array([\"dynamic_feat\",\"dynamic_feat2\"]).alias(\"dynamic_feat\"))\n",
    "test_final_input = model_test_actual_input.select(\"start\",\"target\", f.array([\"cat1\",\"cat2\",\"cat3\"]).alias(\"cat\"))\n",
    "                                                    #f.array([\"dynamic_feat\",\"dynamic_feat2\"]).alias(\"dynamic_feat\"))\n",
    "\n",
    "model_train_w_combo = model_train_w_combo.select(\"Syscode_Stn\",\"start\",\"target\", f.array([\"cat1\",\"cat2\",\"cat3\"]).alias(\"cat\"))\n",
    "                                                    #f.array([\"dynamic_feat\",\"dynamic_feat2\"]).alias(\"dynamic_feat\"))\n",
    "model_test_w_combo = model_test_w_combo.select(\"Syscode_Stn\",\"start\",\"target\", f.array([\"cat1\",\"cat2\",\"cat3\"]).alias(\"cat\"))\n",
    "                                                    #f.array([\"dynamic_feat\",\"dynamic_feat2\"]).alias(\"dynamic_feat\"))\n",
    "\n",
    "train_final_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4581"
     ]
    }
   ],
   "source": [
    "print(model_test_w_combo.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write test and train data sets to JSON format on S3\n",
    "model_test_w_combo.write.mode('overwrite').json('s3://srdata-lab/Matt/DeepARInputs/deepar_input/final_model_input/deepar_4mkt_2mo_test')\n",
    "model_train_w_combo.write.mode('overwrite').json('s3://srdata-lab/Matt/DeepARInputs/deepar_input/final_model_input/deepar_4mkt_2mo_train')\n",
    "\n",
    "test_final_input.write.mode('overwrite').json('s3://srdata-lab/Matt/DeepARInputs/deepar_input/final_model_input/deepar_4mkt_2mo_test_input')\n",
    "train_final_input.write.mode('overwrite').json('s3://srdata-lab/Matt/DeepARInputs/deepar_input/final_model_input/deepar_4mkt_2mo_train_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
